{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries to prepare the data\n",
    "You need numpy and pandas to manipulate the data. If you haven't already, use pip to install these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('arrhythmia.data', header = None)\n",
    "y = df.iloc[:, -1] #The last column is the ground-truth label vector\n",
    "X = df.iloc[:,:-1] #The first to second-last columns are the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute the missing data in the dataset\n",
    "As the arrhythmia dataset has missing values, you need to use Imputer from the sklearn library to assign a suitable value to it. Usually, we assign the mean value of the feature to the missing value which is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "X = imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the dataset\n",
    "As the values of the features can range from negative values like -16 to highly positive values like 371, it is best to normalize the data to avoid certain features overly influencing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset to training and validation datasets\n",
    "Assign 30% of the data randomly to the validation set for cross validation. We use ```random_state = 1``` for consistency in case we need to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting with XGBClassifier\n",
    "We test the results with an untuned XGBClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Train Score: 1.0\n",
      "XGB Val Score: 0.764705882353\n"
     ]
    }
   ],
   "source": [
    "#Load the classifier\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(objective=\"multi:softprob\", nthread=-1)\n",
    "\n",
    "#Fit the classifier to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Predicting the results\n",
    "y_train_xgb = model.predict(X_train)\n",
    "y_pred_xgb = model.predict(X_val)\n",
    "print 'XGB Train Score:', np.mean(y_train == y_train_xgb)\n",
    "print 'XGB Val Score:', np.mean(y_val == y_pred_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting with Random Forest Classifier\n",
    "Using the XGBClassifier, we can see the model has completely overfit the training data. This shows there is a lot of potential for improvement in the XGBClassifier using hyperparameter tuning. We shall now see how a Random Forest Classifier performs in comparison to XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Train Score: 0.981012658228\n",
      "Random Forest Val Score: 0.713235294118\n"
     ]
    }
   ],
   "source": [
    "#Load the classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_jobs = -1, random_state = 0)\n",
    "\n",
    "#Fit the classifier to the training data.\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#Predicting the results\n",
    "y_train_forest = forest.predict(X_train)\n",
    "y_pred_forest = forest.predict(X_val)\n",
    "print 'Random Forest Train Score:', np.mean(y_train == y_train_forest)\n",
    "print 'Random Forest Val Score:', np.mean(y_val == y_pred_forest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting with Logistic Regression\n",
    "We see there is a high variance in the Random Forest model as well, since the training score is significantly higher than the validation score. Nevertheless, a pure XGBClassifier still performs better than a pure Random Forest classifier! What about Logistc Regression, then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Score: 0.990506329114\n",
      "Logistic Regression Val Score: 0.705882352941\n"
     ]
    }
   ],
   "source": [
    "#Load the classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "#Fit the classifier to the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#Predicting the results\n",
    "y_train_lr = lr.predict(X_train)\n",
    "y_pred_lr = lr.predict(X_val)\n",
    "print 'Logistic Regression Train Score:', np.mean(y_train == y_train_lr)\n",
    "print 'Logistic Regression Val Score:', np.mean(y_val == y_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search and Hyperparameter Tuning (XGBClassifier)\n",
    "We see that the Logistic Regression model performs rather well as well, although not as well as the other 2 classifiers. All 3 models have overfit the training data. We can prevent the overfitting by performing cross validation alongside with a random search for the hyperparameters to fine-tune the models. This can be done through the RandomSearchCV function from the sklearn library. \n",
    "\n",
    "In specific, we would like to focus on 3 main aspects of an XGBClassifier for tuning:\n",
    "1. Depth of the trees\n",
    "2. Subsampling ratio to control the variance.\n",
    "3. Learning rate of the classifier\n",
    "\n",
    "We will perform a randomized search with cross validation to tune the XGBClassfier model. Note that this will take quite some time, depending on your computer. My machine took 19.4 minutes to finish computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] subsample=0.384381707293, learning_rate=0.623563696786, max_depth=6 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.857945617623, max_depth=6 \n",
      "[CV] subsample=0.384381707293, learning_rate=0.623563696786, max_depth=6 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.857945617623, max_depth=6 \n",
      "[CV] subsample=0.384381707293, learning_rate=0.847251738784, max_depth=6 \n",
      "[CV] subsample=0.384381707293, learning_rate=0.847251738784, max_depth=6 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.847251738784, max_depth=6 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.847251738784, max_depth=6 \n",
      "[CV] subsample=0.384381707293, learning_rate=0.623563696786, max_depth=1 \n",
      "[CV] subsample=0.384381707293, learning_rate=0.623563696786, max_depth=1 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.623563696786, max_depth=4 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.623563696786, max_depth=4 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.857945617623, max_depth=4 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.857945617623, max_depth=4 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.623563696786, max_depth=6 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.623563696786, max_depth=6 \n",
      "[CV]  subsample=0.297534606544, learning_rate=0.623563696786, max_depth=4 -   1.0s\n",
      "[CV] subsample=0.384381707293, learning_rate=0.847251738784, max_depth=1 \n",
      "[CV]  subsample=0.384381707293, learning_rate=0.623563696786, max_depth=1 -   1.1s\n",
      "[CV]  subsample=0.297534606544, learning_rate=0.857945617623, max_depth=4 -   1.1s\n",
      "[CV] subsample=0.297534606544, learning_rate=0.847251738784, max_depth=1 \n",
      "[CV]  subsample=0.297534606544, learning_rate=0.623563696786, max_depth=4 -   1.1s\n",
      "[CV] subsample=0.384381707293, learning_rate=0.847251738784, max_depth=1 \n",
      "[CV] subsample=0.297534606544, learning_rate=0.847251738784, max_depth=1 \n",
      "[CV]  subsample=0.384381707293, learning_rate=0.623563696786, max_depth=1 -   1.4s\n",
      "[CV]  subsample=0.297534606544, learning_rate=0.857945617623, max_depth=4 -   1.4s\n",
      "[CV]  subsample=0.297534606544, learning_rate=0.623563696786, max_depth=6 -   1.4s\n",
      "[CV]  subsample=0.297534606544, learning_rate=0.623563696786, max_depth=6 -   1.6s\n",
      "[CV]  subsample=0.384381707293, learning_rate=0.847251738784, max_depth=1 -   1.0s\n",
      "[CV]  subsample=0.297534606544, learning_rate=0.847251738784, max_depth=1 -   0.9s\n",
      "[CV]  subsample=0.297534606544, learning_rate=0.847251738784, max_depth=1 -   0.9s\n",
      "[CV]  subsample=0.384381707293, learning_rate=0.847251738784, max_depth=1 -   0.9s\n"
     ]
    }
   ],
   "source": [
    "#Set the random seed for consistency of results\n",
    "np.random.seed(0)\n",
    "\n",
    "#Load the classifier\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(nthread=-1)\n",
    "\n",
    "#Set up a new classifier that will randomly search through the hyperparameters\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from numpy.random import randint, random_sample\n",
    "clf = RandomizedSearchCV(\n",
    "    model,\n",
    "    {\n",
    "        'max_depth':randint(1,10,3),\n",
    "        'learning_rate': random_sample(3),\n",
    "        'subsample': random_sample(2)\n",
    "    },\n",
    "    cv=2,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print \"Done\"\n",
    "#Fit the classifier to the training data, which will randomly search for hyperparameters\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print \"DONE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predicting the results\n",
    "y_train_xgb = clf.predict(X_train)\n",
    "y_pred_xgb = clf.predict(X_val)\n",
    "print 'XGB Train Score:', np.mean(y_train == y_train_xgb)\n",
    "print 'XGB Val Score:', np.mean(y_val == y_pred_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
